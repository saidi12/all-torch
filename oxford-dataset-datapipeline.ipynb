{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import scipy\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordDataset(Dataset):\n",
    "    def __init__(self, rootdir,transform):\n",
    "        self.rootdir = rootdir\n",
    "        self.img_dir = os.path.join(rootdir, \"jpg\")\n",
    "        \n",
    "        labels_mat = scipy.io.loadmat(os.path.join(self.rootdir, \"imagelabels.mat\"))\n",
    "        self.labels = labels_mat['labels'][0] - 1\n",
    "        self.transform = transform\n",
    "        self.error_logs = []\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        try:\n",
    "            img_name = f'image_{idx + 1:05d}.jpg'\n",
    "\n",
    "            img_path = os.path.join(self.img_dir, img_name)\n",
    "            image = Image.open(img_path)\n",
    "            image.verify()\n",
    "            image = Image.open(img_path)\n",
    "            if image.size[0] < 32 or image.size[1] <32:\n",
    "                raise ValueError(f\"Image too small {image.size}\")\n",
    "            \n",
    "            if image.mode != \"RGB\":\n",
    "                image = image.convert('RGB')\n",
    "            \n",
    "            image = self.transform(image)\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            self.error_logs.append({\n",
    "                'index':idx,\n",
    "                'error':e,\n",
    "                'path': img_path if 'img_path' in locals() else 'unknown'})\n",
    "            next_idx = (idx+1) % len(self)\n",
    "            return self.__getitem__(next_idx)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OxfordDataset(rootdir=\"flower_data\", transform=transform)\n",
    "dataloader = DataLoader(dataset, 4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(len(dataset) * 0.70)\n",
    "validation_split = int(len(dataset) * 0.15)\n",
    "test_split = len(dataset) - train_split - validation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 5732test : 1229validation : 1228\n"
     ]
    }
   ],
   "source": [
    "print(f\"train : {train_split}\"\n",
    "      f\"test : {test_split}\"\n",
    "      f\"validation : {validation_split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset,test_dataset = random_split(dataset, [train_split,validation_split,test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5732"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1229"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset,batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset,batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for images,labels in train_loader:\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(150528, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,102)\n",
    "\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OxfordClassifier().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer,loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx,(data,labels) in enumerate(train_loader):\n",
    "        data,labels = data.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _,predicted = output.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        if batch_idx %  10 == 0 and batch_idx > 0:\n",
    "\n",
    "            #validation check\n",
    "            avg_loss = running_loss/10\n",
    "            acc = 100. * correct/total\n",
    "            print(f' [{batch_idx * len(data)}/{len(train_loader.dataset)}]'\n",
    "                  f' Loss: {avg_loss:.3f} | Accuracy: {acc:.1f} %')\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model,validation_loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, targets) in enumerate(validation_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output = model(data)\n",
    "            _,predicted = output.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return 100. * correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      " [320/5732] Loss: 15.389 | Accuracy: 10.5 %\n",
      " [640/5732] Loss: 15.083 | Accuracy: 12.1 %\n",
      " [960/5732] Loss: 16.215 | Accuracy: 11.2 %\n",
      " [1280/5732] Loss: 14.991 | Accuracy: 11.4 %\n",
      " [1600/5732] Loss: 13.056 | Accuracy: 11.6 %\n",
      " [1920/5732] Loss: 15.873 | Accuracy: 11.1 %\n",
      " [2240/5732] Loss: 13.307 | Accuracy: 10.7 %\n",
      " [2560/5732] Loss: 14.786 | Accuracy: 10.6 %\n",
      " [2880/5732] Loss: 18.481 | Accuracy: 10.8 %\n",
      " [3200/5732] Loss: 16.509 | Accuracy: 10.5 %\n",
      " [3520/5732] Loss: 14.521 | Accuracy: 10.2 %\n",
      " [3840/5732] Loss: 25.419 | Accuracy: 10.0 %\n",
      " [4160/5732] Loss: 16.188 | Accuracy: 9.9 %\n",
      " [4480/5732] Loss: 18.449 | Accuracy: 9.9 %\n",
      " [4800/5732] Loss: 20.485 | Accuracy: 9.9 %\n",
      " [5120/5732] Loss: 15.129 | Accuracy: 9.9 %\n",
      " [5440/5732] Loss: 21.304 | Accuracy: 9.9 %\n",
      "Validation Accuracy : 8.333333333333334\n",
      "Epoch : 1\n",
      " [320/5732] Loss: 11.479 | Accuracy: 15.9 %\n",
      " [640/5732] Loss: 23.339 | Accuracy: 14.6 %\n",
      " [960/5732] Loss: 11.851 | Accuracy: 14.4 %\n",
      " [1280/5732] Loss: 11.672 | Accuracy: 14.4 %\n",
      " [1600/5732] Loss: 11.944 | Accuracy: 15.1 %\n",
      " [1920/5732] Loss: 17.233 | Accuracy: 15.4 %\n",
      " [2240/5732] Loss: 11.198 | Accuracy: 15.5 %\n",
      " [2560/5732] Loss: 11.663 | Accuracy: 15.5 %\n",
      " [2880/5732] Loss: 9.448 | Accuracy: 15.4 %\n",
      " [3200/5732] Loss: 10.368 | Accuracy: 15.0 %\n",
      " [3520/5732] Loss: 12.344 | Accuracy: 14.8 %\n",
      " [3840/5732] Loss: 12.416 | Accuracy: 15.1 %\n",
      " [4160/5732] Loss: 8.851 | Accuracy: 15.0 %\n",
      " [4480/5732] Loss: 9.939 | Accuracy: 14.7 %\n",
      " [4800/5732] Loss: 10.049 | Accuracy: 14.9 %\n",
      " [5120/5732] Loss: 11.748 | Accuracy: 14.8 %\n",
      " [5440/5732] Loss: 11.276 | Accuracy: 14.6 %\n",
      "Validation Accuracy : 8.333333333333334\n",
      "Epoch : 2\n",
      " [320/5732] Loss: 12.202 | Accuracy: 14.5 %\n",
      " [640/5732] Loss: 12.428 | Accuracy: 15.5 %\n",
      " [960/5732] Loss: 8.534 | Accuracy: 16.5 %\n",
      " [1280/5732] Loss: 10.054 | Accuracy: 17.1 %\n",
      " [1600/5732] Loss: 11.132 | Accuracy: 17.2 %\n",
      " [1920/5732] Loss: 14.096 | Accuracy: 16.9 %\n",
      " [2240/5732] Loss: 12.596 | Accuracy: 16.7 %\n",
      " [2560/5732] Loss: 6.743 | Accuracy: 16.8 %\n",
      " [2880/5732] Loss: 8.535 | Accuracy: 17.0 %\n",
      " [3200/5732] Loss: 10.955 | Accuracy: 16.9 %\n",
      " [3520/5732] Loss: 10.885 | Accuracy: 16.8 %\n",
      " [3840/5732] Loss: 10.882 | Accuracy: 16.6 %\n",
      " [4160/5732] Loss: 10.741 | Accuracy: 16.4 %\n",
      " [4480/5732] Loss: 11.748 | Accuracy: 16.3 %\n",
      " [4800/5732] Loss: 12.042 | Accuracy: 16.3 %\n",
      " [5120/5732] Loss: 10.247 | Accuracy: 16.4 %\n",
      " [5440/5732] Loss: 10.397 | Accuracy: 16.3 %\n",
      "Validation Accuracy : 0.0\n",
      "Epoch : 3\n",
      " [320/5732] Loss: 12.827 | Accuracy: 12.8 %\n",
      " [640/5732] Loss: 20.728 | Accuracy: 15.3 %\n",
      " [960/5732] Loss: 13.770 | Accuracy: 15.5 %\n",
      " [1280/5732] Loss: 12.744 | Accuracy: 17.3 %\n",
      " [1600/5732] Loss: 13.837 | Accuracy: 17.5 %\n",
      " [1920/5732] Loss: 10.608 | Accuracy: 17.7 %\n",
      " [2240/5732] Loss: 16.652 | Accuracy: 17.3 %\n",
      " [2560/5732] Loss: 9.954 | Accuracy: 17.2 %\n",
      " [2880/5732] Loss: 19.293 | Accuracy: 16.6 %\n",
      " [3200/5732] Loss: 13.454 | Accuracy: 16.5 %\n",
      " [3520/5732] Loss: 12.417 | Accuracy: 16.1 %\n",
      " [3840/5732] Loss: 12.987 | Accuracy: 16.2 %\n",
      " [4160/5732] Loss: 8.997 | Accuracy: 16.1 %\n",
      " [4480/5732] Loss: 10.167 | Accuracy: 16.2 %\n",
      " [4800/5732] Loss: 10.097 | Accuracy: 16.3 %\n",
      " [5120/5732] Loss: 11.081 | Accuracy: 16.3 %\n",
      " [5440/5732] Loss: 12.955 | Accuracy: 16.2 %\n",
      "Validation Accuracy : 8.333333333333334\n",
      "Epoch : 4\n",
      " [320/5732] Loss: 8.967 | Accuracy: 20.7 %\n",
      " [640/5732] Loss: 10.221 | Accuracy: 21.0 %\n",
      " [960/5732] Loss: 8.633 | Accuracy: 20.9 %\n",
      " [1280/5732] Loss: 7.905 | Accuracy: 20.4 %\n",
      " [1600/5732] Loss: 7.716 | Accuracy: 20.6 %\n",
      " [1920/5732] Loss: 7.001 | Accuracy: 20.6 %\n",
      " [2240/5732] Loss: 7.620 | Accuracy: 20.8 %\n",
      " [2560/5732] Loss: 7.656 | Accuracy: 20.8 %\n",
      " [2880/5732] Loss: 7.567 | Accuracy: 20.6 %\n",
      " [3200/5732] Loss: 12.438 | Accuracy: 20.5 %\n",
      " [3520/5732] Loss: 9.821 | Accuracy: 20.3 %\n",
      " [3840/5732] Loss: 10.584 | Accuracy: 20.3 %\n",
      " [4160/5732] Loss: 11.867 | Accuracy: 20.2 %\n",
      " [4480/5732] Loss: 8.731 | Accuracy: 19.8 %\n",
      " [4800/5732] Loss: 14.069 | Accuracy: 19.8 %\n",
      " [5120/5732] Loss: 12.896 | Accuracy: 19.8 %\n",
      " [5440/5732] Loss: 22.659 | Accuracy: 19.6 %\n",
      "Validation Accuracy : 25.0\n",
      "Epoch : 5\n",
      " [320/5732] Loss: 10.978 | Accuracy: 21.9 %\n",
      " [640/5732] Loss: 10.426 | Accuracy: 21.6 %\n",
      " [960/5732] Loss: 18.105 | Accuracy: 21.3 %\n",
      " [1280/5732] Loss: 7.682 | Accuracy: 21.8 %\n",
      " [1600/5732] Loss: 7.434 | Accuracy: 21.9 %\n",
      " [1920/5732] Loss: 8.239 | Accuracy: 21.7 %\n",
      " [2240/5732] Loss: 8.032 | Accuracy: 21.7 %\n",
      " [2560/5732] Loss: 13.883 | Accuracy: 21.5 %\n",
      " [2880/5732] Loss: 7.621 | Accuracy: 21.5 %\n",
      " [3200/5732] Loss: 15.617 | Accuracy: 21.4 %\n",
      " [3520/5732] Loss: 10.036 | Accuracy: 21.2 %\n",
      " [3840/5732] Loss: 8.478 | Accuracy: 20.8 %\n",
      " [4160/5732] Loss: 8.567 | Accuracy: 21.2 %\n",
      " [4480/5732] Loss: 8.599 | Accuracy: 20.9 %\n",
      " [4800/5732] Loss: 12.181 | Accuracy: 21.0 %\n",
      " [5120/5732] Loss: 13.214 | Accuracy: 21.0 %\n",
      " [5440/5732] Loss: 16.734 | Accuracy: 20.8 %\n",
      "Validation Accuracy : 8.333333333333334\n",
      "Epoch : 6\n",
      " [320/5732] Loss: 12.136 | Accuracy: 21.0 %\n",
      " [640/5732] Loss: 7.477 | Accuracy: 21.4 %\n",
      " [960/5732] Loss: 8.377 | Accuracy: 21.5 %\n",
      " [1280/5732] Loss: 11.256 | Accuracy: 21.8 %\n",
      " [1600/5732] Loss: 12.039 | Accuracy: 21.7 %\n",
      " [1920/5732] Loss: 11.849 | Accuracy: 21.4 %\n",
      " [2240/5732] Loss: 10.455 | Accuracy: 21.1 %\n",
      " [2560/5732] Loss: 14.998 | Accuracy: 20.8 %\n",
      " [2880/5732] Loss: 11.072 | Accuracy: 20.9 %\n",
      " [3200/5732] Loss: 13.729 | Accuracy: 20.6 %\n",
      " [3520/5732] Loss: 9.238 | Accuracy: 20.6 %\n",
      " [3840/5732] Loss: 10.829 | Accuracy: 20.6 %\n",
      " [4160/5732] Loss: 12.064 | Accuracy: 20.7 %\n",
      " [4480/5732] Loss: 6.758 | Accuracy: 20.6 %\n",
      " [4800/5732] Loss: 11.136 | Accuracy: 21.0 %\n",
      " [5120/5732] Loss: 10.689 | Accuracy: 21.0 %\n",
      " [5440/5732] Loss: 9.156 | Accuracy: 20.9 %\n",
      "Validation Accuracy : 8.333333333333334\n",
      "Epoch : 7\n",
      " [320/5732] Loss: 6.817 | Accuracy: 28.7 %\n",
      " [640/5732] Loss: 9.151 | Accuracy: 24.7 %\n",
      " [960/5732] Loss: 7.107 | Accuracy: 23.8 %\n",
      " [1280/5732] Loss: 5.194 | Accuracy: 24.2 %\n",
      " [1600/5732] Loss: 11.694 | Accuracy: 24.0 %\n",
      " [1920/5732] Loss: 9.204 | Accuracy: 22.7 %\n",
      " [2240/5732] Loss: 8.031 | Accuracy: 23.1 %\n",
      " [2560/5732] Loss: 21.025 | Accuracy: 22.7 %\n",
      " [2880/5732] Loss: 5.327 | Accuracy: 22.8 %\n",
      " [3200/5732] Loss: 7.408 | Accuracy: 23.0 %\n",
      " [3520/5732] Loss: 8.594 | Accuracy: 22.9 %\n",
      " [3840/5732] Loss: 5.857 | Accuracy: 22.8 %\n",
      " [4160/5732] Loss: 10.329 | Accuracy: 22.7 %\n",
      " [4480/5732] Loss: 7.745 | Accuracy: 22.5 %\n",
      " [4800/5732] Loss: 6.099 | Accuracy: 22.5 %\n",
      " [5120/5732] Loss: 10.165 | Accuracy: 22.4 %\n",
      " [5440/5732] Loss: 9.512 | Accuracy: 22.3 %\n",
      "Validation Accuracy : 16.666666666666668\n",
      "Epoch : 8\n",
      " [320/5732] Loss: 9.127 | Accuracy: 23.0 %\n",
      " [640/5732] Loss: 13.705 | Accuracy: 22.0 %\n",
      " [960/5732] Loss: 11.267 | Accuracy: 23.3 %\n",
      " [1280/5732] Loss: 8.685 | Accuracy: 22.0 %\n",
      " [1600/5732] Loss: 10.990 | Accuracy: 21.8 %\n",
      " [1920/5732] Loss: 12.396 | Accuracy: 21.8 %\n",
      " [2240/5732] Loss: 10.131 | Accuracy: 22.1 %\n",
      " [2560/5732] Loss: 11.690 | Accuracy: 22.4 %\n",
      " [2880/5732] Loss: 6.437 | Accuracy: 22.2 %\n",
      " [3200/5732] Loss: 7.873 | Accuracy: 22.3 %\n",
      " [3520/5732] Loss: 13.136 | Accuracy: 22.0 %\n",
      " [3840/5732] Loss: 5.923 | Accuracy: 22.1 %\n",
      " [4160/5732] Loss: 6.827 | Accuracy: 22.3 %\n",
      " [4480/5732] Loss: 11.190 | Accuracy: 22.3 %\n",
      " [4800/5732] Loss: 5.848 | Accuracy: 22.5 %\n",
      " [5120/5732] Loss: 23.341 | Accuracy: 22.6 %\n",
      " [5440/5732] Loss: 14.436 | Accuracy: 22.4 %\n",
      "Validation Accuracy : 8.333333333333334\n",
      "Epoch : 9\n",
      " [320/5732] Loss: 11.965 | Accuracy: 26.1 %\n",
      " [640/5732] Loss: 7.966 | Accuracy: 25.0 %\n",
      " [960/5732] Loss: 12.851 | Accuracy: 24.5 %\n",
      " [1280/5732] Loss: 9.959 | Accuracy: 25.1 %\n",
      " [1600/5732] Loss: 9.918 | Accuracy: 25.1 %\n",
      " [1920/5732] Loss: 9.132 | Accuracy: 25.1 %\n",
      " [2240/5732] Loss: 8.057 | Accuracy: 25.3 %\n",
      " [2560/5732] Loss: 15.199 | Accuracy: 25.4 %\n",
      " [2880/5732] Loss: 7.359 | Accuracy: 25.3 %\n",
      " [3200/5732] Loss: 6.067 | Accuracy: 25.4 %\n",
      " [3520/5732] Loss: 7.898 | Accuracy: 25.0 %\n",
      " [3840/5732] Loss: 6.778 | Accuracy: 24.7 %\n",
      " [4160/5732] Loss: 8.344 | Accuracy: 24.7 %\n",
      " [4480/5732] Loss: 7.536 | Accuracy: 24.6 %\n",
      " [4800/5732] Loss: 10.955 | Accuracy: 24.3 %\n",
      " [5120/5732] Loss: 10.532 | Accuracy: 24.2 %\n",
      " [5440/5732] Loss: 13.136 | Accuracy: 24.4 %\n",
      "Validation Accuracy : 8.333333333333334\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for idx in range(num_epochs):\n",
    "    print(f\"Epoch : {idx}\")\n",
    "    train_epoch(model,train_loader,optimizer,loss_function)\n",
    "    validation_acc = validation(model, validation_loader, device)\n",
    "    print(f\"Validation Accuracy : {validation_acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
