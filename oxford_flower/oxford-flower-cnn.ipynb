{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "\n",
    "from utils import OxfordDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OxfordDataset(\"flower_data\", transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4568, -0.4739, -0.4739,  ...,  0.1426,  0.1426,  0.0912],\n",
       "          [-0.4397, -0.4739, -0.4739,  ...,  0.2282,  0.2453,  0.2624],\n",
       "          [-0.3883, -0.4226, -0.4568,  ...,  0.3652,  0.3481,  0.3309],\n",
       "          ...,\n",
       "          [-1.2959, -1.2959, -1.3130,  ..., -1.3815, -1.4158, -1.4500],\n",
       "          [-1.3815, -1.3815, -1.3644,  ..., -1.3302, -1.2959, -1.3130],\n",
       "          [-1.4500, -1.4500, -1.4500,  ..., -1.1075, -1.0733, -1.1247]],\n",
       " \n",
       "         [[ 0.4853,  0.5203,  0.5728,  ...,  0.5203,  0.5028,  0.4328],\n",
       "          [ 0.5378,  0.5553,  0.5903,  ...,  0.6954,  0.6604,  0.6078],\n",
       "          [ 0.5903,  0.5728,  0.5903,  ...,  0.8529,  0.8179,  0.7654],\n",
       "          ...,\n",
       "          [-1.0028, -1.0028, -1.0028,  ..., -1.1954, -1.2129, -1.2479],\n",
       "          [-1.0903, -1.0903, -1.1253,  ..., -1.0553, -1.0553, -1.1429],\n",
       "          [-1.1779, -1.1779, -1.1954,  ..., -0.8277, -0.8452, -0.9678]],\n",
       " \n",
       "         [[-1.4733, -1.5256, -1.5953,  ..., -0.3578, -0.3578, -0.4101],\n",
       "          [-1.4559, -1.5430, -1.5779,  ..., -0.2010, -0.1661, -0.1312],\n",
       "          [-1.4036, -1.4733, -1.5430,  ..., -0.0615, -0.0615, -0.0441],\n",
       "          ...,\n",
       "          [-1.0724, -1.0724, -1.0724,  ..., -1.5779, -1.5430, -1.5430],\n",
       "          [-1.1247, -1.1421, -1.1596,  ..., -1.2990, -1.1944, -1.2467],\n",
       "          [-1.2293, -1.2467, -1.2467,  ..., -1.1073, -0.9853, -1.0550]]]),\n",
       " np.uint8(76))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = int(0.70 * len(dataset))\n",
    "validation_split = int(0.15 * len(dataset))\n",
    "test_split = len(dataset) - train_split - validation_split\n",
    "\n",
    "train_dataset, validation_split, test_split = random_split(dataset, [train_split, validation_split, test_split])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_split,batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_split, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([101,  76,  91,  55,  77,  74,  97,  23, 101,  21,  86,  76,   3,  24,\n",
      "         64,  89,   1, 101,  54,  29, 101,  10,  22,  14,  17,  40,  73,  22,\n",
      "         77,  10,  93,  51], dtype=torch.uint8)\n",
      "101\n",
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for image,label in train_loader:\n",
    "    print(label)\n",
    "    print(label[0].item())\n",
    "    print(image.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32,kernel_size=3,padding=1,stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=512,kernel_size=3,stride=1,padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(512*7*7,1024)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(1024,102)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)  \n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.0005, weight_decay=0.0005)\n",
    "loss_function = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_function):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:  # Changed condition\n",
    "            avg_loss = running_loss / 10  # Now correct\n",
    "            acc = 100. * correct / total\n",
    "            print(f' [{(batch_idx + 1) * len(data)}/{len(train_loader.dataset)}]'\n",
    "                  f' Loss: {avg_loss:.3f} | Accuracy: {acc:.1f} %')\n",
    "            running_loss = 0.0  # Only reset loss, keep accuracy cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model,validation_loader,device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, targets) in enumerate(validation_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            output = model(data)\n",
    "            _,predicted = output.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return 100. * correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0\n",
      " [320/5732] Loss: 1.139 | Accuracy: 66.6 %\n",
      " [640/5732] Loss: 1.238 | Accuracy: 65.8 %\n",
      " [960/5732] Loss: 1.096 | Accuracy: 66.9 %\n",
      " [1280/5732] Loss: 1.034 | Accuracy: 67.6 %\n",
      " [1600/5732] Loss: 1.058 | Accuracy: 67.9 %\n",
      " [1920/5732] Loss: 1.109 | Accuracy: 67.9 %\n",
      " [2240/5732] Loss: 1.017 | Accuracy: 67.8 %\n",
      " [2560/5732] Loss: 1.106 | Accuracy: 67.9 %\n",
      " [2880/5732] Loss: 1.049 | Accuracy: 67.9 %\n",
      " [3200/5732] Loss: 1.118 | Accuracy: 67.7 %\n",
      " [3520/5732] Loss: 1.060 | Accuracy: 67.9 %\n",
      " [3840/5732] Loss: 1.105 | Accuracy: 67.9 %\n",
      " [4160/5732] Loss: 1.241 | Accuracy: 67.8 %\n",
      " [4480/5732] Loss: 1.208 | Accuracy: 67.5 %\n",
      " [4800/5732] Loss: 1.268 | Accuracy: 67.2 %\n",
      " [5120/5732] Loss: 1.234 | Accuracy: 67.1 %\n",
      " [5440/5732] Loss: 1.072 | Accuracy: 67.1 %\n",
      " [720/5732] Loss: 1.351 | Accuracy: 67.1 %\n",
      "Validation Accuracy : 57.899022801302934\n",
      "Epoch : 1\n",
      " [320/5732] Loss: 1.015 | Accuracy: 71.6 %\n",
      " [640/5732] Loss: 0.916 | Accuracy: 73.0 %\n",
      " [960/5732] Loss: 0.975 | Accuracy: 71.8 %\n",
      " [1280/5732] Loss: 0.924 | Accuracy: 72.1 %\n",
      " [1600/5732] Loss: 0.867 | Accuracy: 72.4 %\n",
      " [1920/5732] Loss: 1.022 | Accuracy: 71.8 %\n",
      " [2240/5732] Loss: 1.026 | Accuracy: 71.7 %\n",
      " [2560/5732] Loss: 0.991 | Accuracy: 71.3 %\n",
      " [2880/5732] Loss: 0.969 | Accuracy: 71.2 %\n",
      " [3200/5732] Loss: 1.014 | Accuracy: 71.1 %\n",
      " [3520/5732] Loss: 1.088 | Accuracy: 70.6 %\n",
      " [3840/5732] Loss: 0.900 | Accuracy: 70.9 %\n",
      " [4160/5732] Loss: 1.039 | Accuracy: 70.7 %\n",
      " [4480/5732] Loss: 0.914 | Accuracy: 71.0 %\n",
      " [4800/5732] Loss: 0.861 | Accuracy: 71.4 %\n",
      " [5120/5732] Loss: 0.963 | Accuracy: 71.4 %\n",
      " [5440/5732] Loss: 1.006 | Accuracy: 71.4 %\n",
      " [720/5732] Loss: 1.113 | Accuracy: 71.3 %\n",
      "Validation Accuracy : 58.306188925081436\n",
      "Epoch : 2\n",
      " [320/5732] Loss: 0.863 | Accuracy: 75.0 %\n",
      " [640/5732] Loss: 0.926 | Accuracy: 73.4 %\n",
      " [960/5732] Loss: 0.729 | Accuracy: 75.1 %\n",
      " [1280/5732] Loss: 0.871 | Accuracy: 75.4 %\n",
      " [1600/5732] Loss: 0.712 | Accuracy: 76.1 %\n",
      " [1920/5732] Loss: 0.734 | Accuracy: 76.2 %\n",
      " [2240/5732] Loss: 1.053 | Accuracy: 75.6 %\n",
      " [2560/5732] Loss: 0.818 | Accuracy: 75.4 %\n",
      " [2880/5732] Loss: 0.850 | Accuracy: 75.3 %\n",
      " [3200/5732] Loss: 0.847 | Accuracy: 75.4 %\n",
      " [3520/5732] Loss: 0.835 | Accuracy: 75.0 %\n",
      " [3840/5732] Loss: 0.925 | Accuracy: 74.9 %\n",
      " [4160/5732] Loss: 0.625 | Accuracy: 75.3 %\n",
      " [4480/5732] Loss: 0.731 | Accuracy: 75.3 %\n",
      " [4800/5732] Loss: 0.975 | Accuracy: 75.1 %\n",
      " [5120/5732] Loss: 0.827 | Accuracy: 75.1 %\n",
      " [5440/5732] Loss: 0.904 | Accuracy: 74.9 %\n",
      " [720/5732] Loss: 0.952 | Accuracy: 74.8 %\n",
      "Validation Accuracy : 60.504885993485345\n",
      "Epoch : 3\n",
      " [320/5732] Loss: 0.652 | Accuracy: 81.9 %\n",
      " [640/5732] Loss: 0.660 | Accuracy: 81.4 %\n",
      " [960/5732] Loss: 0.618 | Accuracy: 81.4 %\n",
      " [1280/5732] Loss: 0.655 | Accuracy: 81.6 %\n",
      " [1600/5732] Loss: 0.705 | Accuracy: 81.2 %\n",
      " [1920/5732] Loss: 0.558 | Accuracy: 81.7 %\n",
      " [2240/5732] Loss: 0.635 | Accuracy: 81.3 %\n",
      " [2560/5732] Loss: 0.703 | Accuracy: 81.1 %\n",
      " [2880/5732] Loss: 0.667 | Accuracy: 81.1 %\n",
      " [3200/5732] Loss: 0.663 | Accuracy: 81.0 %\n",
      " [3520/5732] Loss: 0.605 | Accuracy: 81.0 %\n",
      " [3840/5732] Loss: 0.759 | Accuracy: 80.7 %\n",
      " [4160/5732] Loss: 0.772 | Accuracy: 80.5 %\n",
      " [4480/5732] Loss: 0.869 | Accuracy: 79.9 %\n",
      " [4800/5732] Loss: 0.822 | Accuracy: 79.6 %\n",
      " [5120/5732] Loss: 0.815 | Accuracy: 79.4 %\n",
      " [5440/5732] Loss: 0.664 | Accuracy: 79.4 %\n",
      " [720/5732] Loss: 0.723 | Accuracy: 79.3 %\n",
      "Validation Accuracy : 58.469055374592834\n",
      "Epoch : 4\n",
      " [320/5732] Loss: 0.591 | Accuracy: 81.6 %\n",
      " [640/5732] Loss: 0.514 | Accuracy: 83.3 %\n",
      " [960/5732] Loss: 0.518 | Accuracy: 83.1 %\n",
      " [1280/5732] Loss: 0.631 | Accuracy: 83.0 %\n",
      " [1600/5732] Loss: 0.711 | Accuracy: 82.2 %\n",
      " [1920/5732] Loss: 0.722 | Accuracy: 81.5 %\n",
      " [2240/5732] Loss: 0.554 | Accuracy: 81.6 %\n",
      " [2560/5732] Loss: 0.622 | Accuracy: 81.4 %\n",
      " [2880/5732] Loss: 0.596 | Accuracy: 81.4 %\n",
      " [3200/5732] Loss: 0.603 | Accuracy: 81.3 %\n",
      " [3520/5732] Loss: 0.640 | Accuracy: 81.1 %\n",
      " [3840/5732] Loss: 0.600 | Accuracy: 81.1 %\n",
      " [4160/5732] Loss: 0.559 | Accuracy: 81.1 %\n",
      " [4480/5732] Loss: 0.545 | Accuracy: 81.4 %\n",
      " [4800/5732] Loss: 0.604 | Accuracy: 81.5 %\n",
      " [5120/5732] Loss: 0.553 | Accuracy: 81.6 %\n",
      " [5440/5732] Loss: 0.657 | Accuracy: 81.4 %\n",
      " [720/5732] Loss: 0.647 | Accuracy: 81.4 %\n",
      "Validation Accuracy : 59.69055374592834\n",
      "Epoch : 5\n",
      " [320/5732] Loss: 0.550 | Accuracy: 84.7 %\n",
      " [640/5732] Loss: 0.626 | Accuracy: 83.0 %\n",
      " [960/5732] Loss: 0.514 | Accuracy: 84.1 %\n",
      " [1280/5732] Loss: 0.500 | Accuracy: 84.4 %\n",
      " [1600/5732] Loss: 0.511 | Accuracy: 84.5 %\n",
      " [1920/5732] Loss: 0.565 | Accuracy: 84.3 %\n",
      " [2240/5732] Loss: 0.500 | Accuracy: 84.4 %\n",
      " [2560/5732] Loss: 0.450 | Accuracy: 84.6 %\n",
      " [2880/5732] Loss: 0.452 | Accuracy: 84.5 %\n",
      " [3200/5732] Loss: 0.543 | Accuracy: 84.3 %\n",
      " [3520/5732] Loss: 0.518 | Accuracy: 84.5 %\n",
      " [3840/5732] Loss: 0.624 | Accuracy: 84.4 %\n",
      " [4160/5732] Loss: 0.616 | Accuracy: 84.0 %\n",
      " [4480/5732] Loss: 0.606 | Accuracy: 83.9 %\n",
      " [4800/5732] Loss: 0.521 | Accuracy: 84.0 %\n",
      " [5120/5732] Loss: 0.508 | Accuracy: 84.0 %\n",
      " [5440/5732] Loss: 0.592 | Accuracy: 83.9 %\n",
      " [720/5732] Loss: 0.505 | Accuracy: 83.9 %\n",
      "Validation Accuracy : 60.83061889250814\n",
      "Epoch : 6\n",
      " [320/5732] Loss: 0.464 | Accuracy: 84.1 %\n",
      " [640/5732] Loss: 0.417 | Accuracy: 85.2 %\n",
      " [960/5732] Loss: 0.480 | Accuracy: 85.2 %\n",
      " [1280/5732] Loss: 0.356 | Accuracy: 86.0 %\n",
      " [1600/5732] Loss: 0.390 | Accuracy: 86.6 %\n",
      " [1920/5732] Loss: 0.475 | Accuracy: 86.5 %\n",
      " [2240/5732] Loss: 0.523 | Accuracy: 86.2 %\n",
      " [2560/5732] Loss: 0.410 | Accuracy: 86.2 %\n",
      " [2880/5732] Loss: 0.495 | Accuracy: 85.9 %\n",
      " [3200/5732] Loss: 0.476 | Accuracy: 85.9 %\n",
      " [3520/5732] Loss: 0.435 | Accuracy: 86.1 %\n",
      " [3840/5732] Loss: 0.527 | Accuracy: 86.0 %\n",
      " [4160/5732] Loss: 0.467 | Accuracy: 85.7 %\n",
      " [4480/5732] Loss: 0.506 | Accuracy: 85.7 %\n",
      " [4800/5732] Loss: 0.547 | Accuracy: 85.5 %\n",
      " [5120/5732] Loss: 0.483 | Accuracy: 85.4 %\n",
      " [5440/5732] Loss: 0.584 | Accuracy: 85.2 %\n",
      " [720/5732] Loss: 0.628 | Accuracy: 85.2 %\n",
      "Validation Accuracy : 60.74918566775244\n",
      "Epoch : 7\n",
      " [320/5732] Loss: 0.462 | Accuracy: 83.8 %\n",
      " [640/5732] Loss: 0.438 | Accuracy: 85.3 %\n",
      " [960/5732] Loss: 0.349 | Accuracy: 87.1 %\n",
      " [1280/5732] Loss: 0.352 | Accuracy: 87.4 %\n",
      " [1600/5732] Loss: 0.406 | Accuracy: 87.1 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     validation_acc = validation(model, validation_loader, device)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation Accuracy : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, loss_function)\u001b[39m\n\u001b[32m      4\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/oxford_flower/utils.py:34\u001b[39m, in \u001b[36mOxfordDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image.mode != \u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     32\u001b[39m     image = image.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:253\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/all-torch/.venv/lib/python3.12/site-packages/PIL/Image.py:2304\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2292\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2294\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2295\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2296\u001b[39m         )\n\u001b[32m   2297\u001b[39m         box = (\n\u001b[32m   2298\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2299\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2300\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2301\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2302\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for idx in range(num_epochs):\n",
    "    print(f\"Epoch : {idx}\")\n",
    "    train_epoch(model,train_loader,optimizer,loss_function)\n",
    "    validation_acc = validation(model, validation_loader, device)\n",
    "    print(f\"Validation Accuracy : {validation_acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
